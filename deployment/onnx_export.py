"""
onnx_export.py â€” Ultra Pro ONNX Exporter for SAM (Enhanced Final Version)

âœ… Exports PyTorch model to ONNX (opset 11+)
âœ… Supports dynamic/static batch size
âœ… Output verification via onnxruntime
âœ… CLI-compatible & importable module
âœ… Logs model file size and uploads to S3/HF
âœ… Uses logging module for clean output
âœ… Suitable for deployment (TensorRT, HF Space)
âœ… CI/test-ready

Author: Your Name
License: MIT
"""

import os
import torch
import argparse
import logging
import onnx
import onnxruntime
import numpy as np

from models.sam_original import SegmentAnythingModel
from utils.upload import upload_fn  # Make sure this exists

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("onnx_export")

def load_model(encoder_type, decoder_type, checkpoint=None):
    model = SegmentAnythingModel(encoder_type=encoder_type, decoder_type=decoder_type)
    if checkpoint:
        state_dict = torch.load(checkpoint, map_location='cpu')
        model.load_state_dict(state_dict)
    return model.eval()

def export_to_onnx(model, output_path="sam_model.onnx", dynamic=False, input_size=(1, 3, 256, 256)):
    dummy_input = torch.randn(*input_size)
    input_names = ["input"]
    output_names = ["output"]

    dynamic_axes = {"input": {0: "batch"}, "output": {0: "batch"}} if dynamic else None

    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        verbose=False,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
        opset_version=11
    )

    logger.info(f"âœ… Exported ONNX model to: {output_path}")
    return output_path

def verify_onnx_model(onnx_path, input_shape=(1, 3, 256, 256)):
    ort_session = onnxruntime.InferenceSession(onnx_path)
    dummy = np.random.randn(*input_shape).astype(np.float32)
    outputs = ort_session.run(None, {"input": dummy})
    logger.info(f"ðŸ§ª ONNX Output Shape: {outputs[0].shape}")
    return outputs[0]

def report_model_size(path):
    size_mb = os.path.getsize(path) / 1024 ** 2
    logger.info(f"ðŸ“¦ ONNX model size: {size_mb:.2f} MB")
    return size_mb

def main(args):
    model = load_model(args.encoder, args.decoder, args.checkpoint)

    onnx_path = export_to_onnx(
        model,
        output_path=args.output,
        dynamic=args.dynamic,
        input_size=tuple(args.input_shape)
    )

    # ðŸŒŸ Optional Enhancements
    report_model_size(onnx_path)
    upload_fn(onnx_path)

    if args.verify:
        verify_onnx_model(onnx_path, input_shape=tuple(args.input_shape))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Export SAM model to ONNX")
    parser.add_argument("--encoder", type=str, required=True, help="SAM encoder type")
    parser.add_argument("--decoder", type=str, required=True, help="SAM decoder type")
    parser.add_argument("--checkpoint", type=str, help="Path to PyTorch model checkpoint")
    parser.add_argument("--output", type=str, default="sam_model.onnx", help="Output ONNX file path")
    parser.add_argument("--input_shape", type=int, nargs=4, default=[1, 3, 256, 256], help="Input shape for dummy tensor")
    parser.add_argument("--dynamic", action="store_true", help="Enable dynamic batch size for ONNX")
    parser.add_argument("--verify", action="store_true", help="Verify ONNX export with onnxruntime")
    args = parser.parse_args()

    main(args)
